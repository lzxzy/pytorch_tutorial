# PyTorch
## DCGAN Tutorial
本章将通过一个例子来了解DCGAN。我们将训练一个GAN 模型来生成新的名人的图片在提供许多真实的名人图片之后。大部分实现代码来自[pytorch/example](https://github.com/pytorch/examples)实现的dcgan。文章中将会对其实现进行解释并解释为什么模型可以有效运行。不必担心，我们不需要预先了解有关GAN的知识。
### Generative Adversarial Networks
### What is GAN?
GANs 是一种框架用来指导深度学习模型捕捉训练数据的分布，然后使得我们生成新的数据与训练数据有相同的数据。GAN是由Goodfellow 在2014第一次提出。他们建立两个不同的模型，一个生成器和一个鉴别器。生成器的任务是生成假的图片使之看起来和训练集中的真图片一样。在训练中，生成器持续尝试欺骗鉴别器通过生成越来越真实的假图片，同时鉴别器的任务是变得更加智能更好的鉴别和区分真实图片和假图片。当训练达到平衡状态时，生成器生成的图片效果如同直接从训练数据集产生的一样，同时鉴别器对生成图片真假置信度为50%。

接下来，我们定义一些符号作为了解鉴别器的开始。定义$x$表示输入的图片数据。$D(x)$表示鉴别器网络输出为一个概率值表示$x$来自训练数据而不是生成器生成的可能性。这里我们将$D(x)$的输入处理成一个HWC 大小为 3×64×64的图片。直观上，当$x$来自训练数据时$D(x)$会很大，当$x$来自生成器生成数据时$D(x)$很小。$D(x)$也可以被当作一个传统的二分类模型。

对于生成器的符号，定义$z$是从标准分布的潜在矢量空间采样的一个向量。$G(z)$表示生成函数，将采样向量$z$映射到数据空间中。生成器$G$的目标是建立来自训练数据($P_{data}$)的分布假设使得可以从建立的分布（$pg$）中生成假图片。

因此，$D(G(z))$是一个概率度量，用来度量生成器$G$的输出的真实程度。在 Goodfellow 的[论文](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf])中，$G$ & $D$在玩一个对抗游戏 $D$尝试最大化正确区分真实信息和假信息($logD(x)$)，$G$尝试最小化$D$对于假信息的预测输出正确分类的概率($log(1-D(G(x)))$)。最终GAN的目标函数为：
$${min\over{G}}{max\over{D}}V(D,G)=E_{x\sim p_{data}}(x)^{[\log{D(x)}]} + E_{z\sim p_{z}}(z)^{[\log{1-D(G(x))}]}$$
理论上，解决这个对抗游戏的目标就是使得$P_g = P_{data}$，并且鉴别器无法判断真伪。但是，GAN系列的收敛性质依然在继续被研究，而且模型并不能总是达到我们所期望的样子。
### What is DCGAN
DCGAN是对原始GAN网络的直接继承，特别的它在鉴别器和生成器中分别使用卷积和反卷积。最初由Radford et. al提出。鉴别器由卷积层，BN层和LeakyReLU层实现。输入是3×64×64的输入图片，输出是一个概率值用来度量输入来自真实分布的概率。生成器由反卷积，BN层和ReLU实现。输入是一个隐向量$z$，输入来自标准分布并且输出是一个3×64×64的RGB图像。反卷积使得向量变换为和图片一样的值。文章中，作者也给出一些技巧关于如何设置优化器，如何计算损失函数，如何初始化模型，都将在下面依次介绍。

### Data
数据准备十分重要，我们将使用ImageFolder 类，需要一个数据集目录的子目录。现在，我们可以创建数据集，建立数据迭代器，设置运行设备，并可视化一部分数据。
### Implementation
在输入参数和数据集准备好后，我们现在可以对网络进行实现。首先从权重初始化策略开始，然后依次讨论生成器，鉴别器，损失函数和训练细节。
#### Weight Initialization
文章中，作者指出所有的模型权重应该从以个标准均值为0标准差0.2的分布被随机初始化。`weights_init`函数接受一个初始化模型为输入参数并重新初始化所有卷积，反卷积和BN 层与上述法则吻合。函数将在实现模型初始化后立即被应用。
#### Generator
生成器$G$，被设计将隐向量映射到数据空间。由于数据为图片，将$z$转换到数据空间意味着最终生成一张和输入图片大小相同的RGB图片。实际上，是由一系列反卷积及其对应的BN ， ReLU层实现。生成器的输出被送入一个tanh函数返回范围为[-1,1]的值。
值得注意的是转换层后存在BN层，这也是DCGAN中最重要的一点。*nz*是输入向量z的长度，*ngf*与特征图大小相关，*nc*为输出通道数。
#### Discriminator
如之前提到的，鉴别器$D$，是一个二分类网络，输入为一张图片输出是对输入来自真实(对应假冒)分布的概率度量。这里，$D$接受一张3 × 64 × 64的输入图片，通过卷积，BN， LeakyReLU的处理，最终通过Sigmoid函数输出概率值。如有必要，这个结构可以增加更多层为了解决问题。文章中提到通过使用滑动卷积而不是池化达到降采样的目的，因为这样可以使得网络学得自己的池化函数。同样的BN 和leaky relu促进梯度流良性传播，对$D & G$的学习十分重要。
#### Loss Functions and Optimizers
在对$D$和$G$进行初始化后，我们将定义网络如何通过损失函数和优化器进行学习。我们将使用二值交叉损失（BCELoss）函数在PyTorch中定义如下：
$$L(x,y) = L = \{l_1,...,l_N\}^T, l_n = [y_n\cdot\log x_n + (1-y_n)\cdot \log (1-x_n)]$$
需要注意，这个函数包含了$\log(D(x))$和$\log(1-D(G(z)))$两部分目标函数。我们可以指定使用BCE等式的哪一部分通过利用$y$。上述将在接下来训练循环中进行实现，但十分重要的一点是理解我们可以选择我们希望计算哪一部分仅通过改变$y$（ground truth）。

接着，我们定义真实值为1 假冒值为0。这些标签将在计算$D$和$G$的损失时被使用。最后，我们构建分别为D和G构建优化器。为了对生成器的学习进程保持追踪，通过生成固定批次的来自高斯分布的隐向量。在训练过程中，我们将周期性输入这些固定噪声给$G$，在经过一次迭代后，将会看到以图像形式输出的噪声。
### Training
最后，我们已经定义了所有GAN网络需要的部分，可以开始训练它。记住一点，训练GAN网络像是一次艺术行为，由于错误的超参数设置，会导致毫无理论解释的崩溃，因此我们可以遵守一些由ganhacks所显示的最佳实现方法。我们将构建名为"为real & fake 构建不同mini-batches"图片，同时调整G网络的目标函数最大化$\log D(G(z))$。训练分为两个主要的部分。第一部分更新鉴别器，第二部分跟新生成器。

**Part1 - Train the Discriminator**

回想前面说的， 训练鉴别器的目标是最大化正确分类给定输入是真实值还是假值。Goodfellow的观点“更新鉴别器通过增加随机梯度”。实际上，我们想最大化$\log(D(x)) + \log(1-D(G(x)))$。由于来自ganhacks的分离mini-batch建议，在D的前向过程，计算损失（$\log(D(x))$）, 然后再计算梯度进行反向传播。接着构建假样本batch通过当前的生成器，对这些样本通过D进行前向计算，计算损失（$\log(1-D(G(z)))$）, 并且累积梯度用来反向传播。现在由于梯度累积同时来自全真和全假批次，我们可以对鉴别器执行优化器。

**Part2 - Train the Generator**

在文章中，我们想训练生成器通过最小化$\log(1-D(G(z)))$来使得生成器生成更好的假数据。如之前提到的，Goodfellow认为不利于产生有效梯度，尤其在早期学习过程。作为一种修正，我们倾向最大化$\log(D(G(z)))$。在代码中通过如下方式实现：从Part1中使用鉴别器区分生成器的输出，计算生成器的损失使用真实标签作为ground truth，通过反向传播计算生成器的梯度，最终使用优化器更新生成器的参数。似乎使用真实标签real-label当作GT来计算损失函数有点反直觉，但这可以使得我们使用我们更想利用的BCELoss的$\log(x)$部分（而不是$\log(1-x)$）。

最后，我们将做一些统计报告并在每个epoch末尾将把我们的固定噪声传给生成器生生成可视化信息对生成器进行追踪。训练统计量有：
+ Loss_D: 鉴别器损失，通过包括全真和全假批次（$\log(D(x)) + log(D(G(z)))$）
+ Loss_G: 生成器损失$\log(D(G(z)))$
+ D(x): 平均输出（每个batch）鉴别器对全真批次。这个值应该开始时理论上接近1然后随着生成器性能变好,值最终趋向于0.5。
+ D(G(z))